{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LessonScikitLearn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOp3Ngz7ZPjgCsx2E5/m53w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertomanfreda/intensive_school_ml/blob/master/LessonScikitLearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H08nOeXWfDV5",
        "colab_type": "text"
      },
      "source": [
        "# Scikit-learn\n",
        "\n",
        "Scikit-learn is an open source machine learning library, which provides a variety of built-in machine learning algorithms and models, called **estimators**. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.\n",
        "\n",
        "The basic workflow with scikit-learn is that you define an **estimator**, then fit it to some data using its **fit** method (which correspond to the **training** of the model) and eventually use it to predict some unknown feature, which is done with the **predict** method.\n",
        "\n",
        "Unless you have previous experiences with ML, you may not know what these models are - that is what the next days of the school are for!\n",
        "However, I will show a very basic example, just so you can get a feeling how how it works.\n",
        "\n",
        "We will use a **Decision Tree** applied to a classic classification problem: try to predict the specie of an Iris based on some characteristics of the petals and the sepals. These will also gives us the opprtunity to see how to load models and split them into test and training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B89UA6H0icYG",
        "colab_type": "text"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "The **sklearn.datasets** module contains a number of classic toy datasets, which are very useful either for learning ML, as well as for performing checks on your models.\n",
        "\n",
        "For this example we will use the Iris plants dataset, which contains 150 entries, each corresponding to an Iris. The Iris are of three different kinds, which are the **classes** we want to predict:\n",
        " \n",
        "  * Iris-Setosa\n",
        "  * Iris-Versicolour\n",
        "  * Iris-Virginica\n",
        "\n",
        "For each entry, we have 4 **features**, which we will use to try and predict the class:\n",
        "\n",
        "  * sepal length in cm\n",
        "  * sepal width in cm\n",
        "  * petal length in cm\n",
        "  * petal width in cm  \n",
        "\n",
        "and a **target** (or **label**) which tells us the class for that entry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdGuFHp8fCFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "import sklearn.datasets\n",
        "\n",
        "# Load the dataset\n",
        "iris_dataset = sklearn.datasets.load_iris()\n",
        "\n",
        "\"\"\" The dataset has 6 main attributes: data, target, target_names,\n",
        "feature_names, DESCR and file_name.\n",
        "\n",
        "- data is an NumPy ndarray of shape n_samples x n_features (in this case\n",
        "150 x 4) which contains the numerical values of the features for each entry. \n",
        "\n",
        "- target is a 1d ndarray of size n_samples, containing the class for each entry\n",
        "expressed as an intger number: 0, 1, or 2\n",
        "\n",
        "- target_names is a ndarray of strings, which tells us the name of each class.\n",
        "\n",
        "- feature_names similarly contains the name of the features\n",
        "\n",
        "- DESCR is s tring describing the dataset\n",
        "\n",
        "- file_name is a string containing the path to the actual file where the data\n",
        "are stored\n",
        "\"\"\"\n",
        "\n",
        "print(iris_dataset.data.shape)\n",
        "print(iris_dataset.target.shape)\n",
        "print(iris_dataset.target_names)\n",
        "print(iris_dataset.feature_names)\n",
        "print(iris_dataset.DESCR)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7f5jvWPxNHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Let's inspect the data, starting with the classes\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "classes_dict = {}\n",
        "for i, name in enumerate(iris_dataset.target_names):\n",
        "    mask = iris_dataset.target == i\n",
        "    iris_class = iris_dataset.target[mask]\n",
        "    classes_dict[name] = len(iris_class)\n",
        "print(classes_dict)\n",
        "plt.pie(classes_dict.values(), labels=classes_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az4z7rMhoZlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now the features\n",
        "\n",
        "# Print the first 10 entries\n",
        "print(iris_dataset.data[:10])\n",
        "\n",
        "# Observe one of the categories - for example the first (sepal length)\n",
        "sepal_length = iris_dataset.data[:, 0]\n",
        "# Calulate its mean and rms\n",
        "mean_sepal_length = iris_dataset.data\n",
        "bins = np.linspace(min(sepal_length), max(sepal_length), 21)\n",
        "plt.figure('sepal_length')\n",
        "plt.hist(sepal_length, bins=bins)\n",
        "plt.xlabel('{} [cm]'.format(iris_dataset.feature_names[0]))\n",
        "plt.ylabel('entries/bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYTrKnJMwrd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "# Let's do some correlation plots\n",
        "# Iterate over all the possible unique pairs of numbers from 0, 1, 2, 3\n",
        "# There are 6 of them: (0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)\n",
        "fig, ax = plt.subplots(2, 3, figsize=(18, 12))\n",
        "for i, (a, b) in enumerate(itertools.combinations(range(4), 2)):\n",
        "    x = iris_dataset.data[:, a]\n",
        "    y = iris_dataset.data[:, b]\n",
        "    _ax = fig.axes[i]\n",
        "    _ax.scatter(x, y)\n",
        "    _ax.set_xlabel('{} [cm]'.format(iris_dataset.feature_names[a]))\n",
        "    _ax.set_ylabel('{} [cm]'.format(iris_dataset.feature_names[b]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC7A-kXH8olU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finally, let's re-do the last correlation plot, petal-length vs petal-width,\n",
        "# using different colors for different classes \n",
        "\n",
        "plt.figure('scatter_coloured', figsize=(8, 6))\n",
        "colors = ['red', 'green', 'blue']\n",
        "for i, name in enumerate(iris_dataset.target_names):\n",
        "    mask = iris_dataset.target == i\n",
        "    x = iris_dataset.data[mask][:, 2] # petal length\n",
        "    y = iris_dataset.data[mask][:, 3] # petal width\n",
        "    plt.scatter(x, y, color=colors[i], label=name)\n",
        "plt.xlabel(iris_dataset.feature_names[2])\n",
        "plt.ylabel(iris_dataset.feature_names[3])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH4TrJ6xxDhx",
        "colab_type": "text"
      },
      "source": [
        "Now let's split our data into a train and test sample. We will train our model on the train sample and test it on the test sample. The split is random (it is always a good idea to shuffle the data before training).\n",
        "\n",
        "scikit-learn offers an handy tool for randomly splitting the sample into train and test: the **train_test_split** function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyR3WlPazFs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.model_selection\n",
        "\"\"\" Split the dataset into training and testing. We will use 80% of it \n",
        "for training and 20% for testing.\n",
        "Note: we use the 'random_state' argument to ensure repeatibility of the\n",
        "results (you can use any number instead of 42, its purpose is just to\n",
        "initialize the pseudo-random number generator).\"\"\"\n",
        "train_features, test_features, train_targets, test_targets  = \\\n",
        "  sklearn.model_selection.train_test_split(iris_dataset.data,\n",
        "                                           iris_dataset.target,\n",
        "                                           test_size = 0.2,\n",
        "                                           random_state=42)\n",
        "print(train_features.shape, test_features.shape, train_targets.shape, \n",
        "      test_targets.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPk3zoZ23byt",
        "colab_type": "text"
      },
      "source": [
        "Now that we have explored the dataset, let's build our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0qWanwz3hW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import tree\n",
        "model = tree.DecisionTreeClassifier(\n",
        "    criterion                = 'gini',\n",
        "    splitter                 = 'best',\n",
        "    max_depth                = 3,\n",
        "    min_samples_split        = 2,\n",
        "    min_samples_leaf         = 1,\n",
        "    min_weight_fraction_leaf = 0.05\n",
        ")\n",
        "\n",
        "# Train it\n",
        "model = model.fit(train_features, train_targets)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow_yTDkl5cXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's draw the tree\n",
        "plt.figure(figsize=(12, 12))\n",
        "tree.plot_tree(model, feature_names=iris_dataset.feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYSxJ4aO6t_r",
        "colab_type": "text"
      },
      "source": [
        "Let's see how good our model is by trying to predict the classes based on the test features and comparing it with the test targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIEqU8Oe65T8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_targets = model.predict(test_features)\n",
        "right_pred = predicted_targets == test_targets\n",
        "# Since True == 1 and False == 0 the sum() of the elements of the mask is the\n",
        "# equal to number of right choices \n",
        "num_right_pred = right_pred.sum()\n",
        "print('Our model got {:d} right prediction out of {:d}'.format(\n",
        "    num_right_pred, len(predicted_targets)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wahAwOFv-fes",
        "colab_type": "text"
      },
      "source": [
        "That is pretty awsome!"
      ]
    }
  ]
}