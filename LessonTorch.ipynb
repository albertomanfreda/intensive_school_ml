{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LessonTorch.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDOUXPvZH/zxFyKiGPa0QT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertomanfreda/intensive_school_ml/blob/master/LessonTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egJNZIci4--j",
        "colab_type": "text"
      },
      "source": [
        "#PyTorch\n",
        "\n",
        "PyTorch is a deep learning framework, mainly developed by the Facebook AI Research (FAIR) group. \n",
        "\n",
        "The fundamental PyTorch element are tensors. PyTorch tensors are similar to NumPy ndarray (in fact they are purposely created to behave as closely as possible to them), but with two additional capabilities:\n",
        "\n",
        "* They are able to keep track of a computational graph and gradient\n",
        "* They allow to perform operation on GPUs instead of CPUs\n",
        "\n",
        "Let's see how to manage PyTorch Tensors and how they are different or similar to NumPy ndarrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZrZ5lc84gfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "\n",
        "t = torch.Tensor([[3, 2],\n",
        "                  [1, 3]])\n",
        "# Internally, the data are stored inside a data attribute\n",
        "print(t.data)\n",
        "# However you don't need to explictly call data most of the time, as you can\n",
        "# access elements and properties directly from the tensor object.\n",
        "\n",
        "# The type of t is torch.FloatTensor (32 bit)\n",
        "print(t.type())\n",
        "# The shape is an object of type torch.Size, rather than a tuple.\n",
        "print(t.shape)\n",
        "# This is the same as ndarray\n",
        "print(len(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOEEjnfZJFS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Other functionalities look just the same, or very similar\n",
        "t1 = torch.Tensor([[3, 2],\n",
        "                   [1, 3]])\n",
        "# Operation along axis\n",
        "print(t1.mean(dim=0))\n",
        "# Create an tensor of a give shape initialized to 0. \n",
        "t2 = torch.zeros((2, 2))\n",
        "# Sum a scalar to all the elements\n",
        "t3 = t2 + 1.5\n",
        "# Element-wise multiplication\n",
        "print(t1 * t2)\n",
        "# Matrix multiplication (equivalent of dot or matmul)\n",
        "print(torch.mm(t1, t3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-E8MzgRIN2G",
        "colab_type": "text"
      },
      "source": [
        "You can easily convert NumPy ndarrays into PyTorch Tensors and vice-versa using the *numpy()* and *from_numpy()* functions. However, such functions do not copy the data: the tensor and the ndarray will share the same memory space. If one changes, the other also  will."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9KADLSHI28w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Create a PyTorch tensor\n",
        "t = torch.zeros((3, 4))\n",
        "# Create a NumPy array from the torch Tensors\n",
        "y = t.numpy()\n",
        "\n",
        "# The tensor and the ndarray share the same memory space. If we change y...\n",
        "y[1, 1] = 5\n",
        "#...t will change as well\n",
        "print(t)\n",
        "\n",
        "# Convert a ndarray to a tensor\n",
        "x = np.full((2, 3), 1.5)\n",
        "z = torch.from_numpy(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoL_o7p8OiLD",
        "colab_type": "text"
      },
      "source": [
        "Note that, by default, NumPy arrays are initialized as np.float64, while PyTorch tensors as torch.FloatTensor, which is a 32 bit type for GPU comatibility.\n",
        "\n",
        "Mixing 32 bits and 64 bits objects during execution will most likely generate errors, so, in order to interoperate between Tensors and ndarrays, PyTorch offers functions to cast between these types.\n",
        "\n",
        "Since these functions do copy the objects, they can be also used to disentangle the NumPy ndarray from the PyTorch tensor. Beware, however, that copying very large arrays is an expensive operation, in terms of memory and CPU/GPU power.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk4F5hDoPTj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Create a PyTorch tensor\n",
        "t = torch.zeros((3, 4))\n",
        "# Cast to double and create a NumPy array out of it\n",
        "y = t.double().numpy()\n",
        "\n",
        "# The tensor and the ndarray do not share the memory space. If we change y...\n",
        "y[1, 1] = 5\n",
        "#...t will stay the same\n",
        "print(t)\n",
        "\n",
        "# Convert a ndarray to a tensor, then downcast it to 32 bit\n",
        "x = np.full((2, 3), 1.5)\n",
        "z = torch.from_numpy(x).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keMXl1TSWCO9",
        "colab_type": "text"
      },
      "source": [
        "## CPU vs GPU operations\n",
        "\n",
        "The CPU (central processing unit) is a general-purpose processor, which runs most of what happens inside your computer. It is mostly designed for sequential operations, though a certain degree of parallelization is present.\n",
        "\n",
        "A GPU (graphics processing unit), on the other side, is a specialized type of microprocessor which is extremely good at performing heavy operations, like floating point arithmetic, in a highly parallelized way. Though they are primarily designed for quick image rendering, they have also become a popular tool for a variety of data science tasks, including ML.\n",
        "\n",
        "PyTorch supports GPU operations through CUDA, a parallel computing platform developed by NVIDIA. That means that an NVIDIA video card is required for GPU computing to work. The PyTorch module which supports CUDA operations is called (unsurprisingly) **cuda**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNhupSlaX8kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "# Print the number of GPUs available\n",
        "print('Number of CUDA Devices:', torch.cuda.device_count())\n",
        "\n",
        "# Print the name of the GPUs available\n",
        "for i in range(torch.cuda.device_count()):\n",
        "  print('\\tCUDA device: ', torch.cuda.get_device_name(i))\n",
        "print ('Current cuda device number: ', torch.cuda.current_device())\n",
        "\n",
        "# Check if the GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    #Assign cuda GPU located at location '0' to a variable, so we can refer to it\n",
        "    cuda0 = torch.device('cuda:0')\n",
        "    #Performing the addition on GPU\n",
        "    a = torch.ones(3, 2, device=cuda0) # creating a tensor 'a' on GPU\n",
        "    b = torch.ones(3, 2).to(cuda0) # creating a tensor 'b' and move it on GPU\n",
        "    c = a + b # c will be created automatically on GPU\n",
        "    print(c)\n",
        "    # Copy c to CPU\n",
        "    d = c.cpu()\n",
        "    print(d)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtXsqRxGomUw",
        "colab_type": "text"
      },
      "source": [
        "Is GPU really more powerful? Let's test that with an example similar to the NumPy one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRVrKdkSk8dV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def expensive_operation(device, num_trials=5):\n",
        "    \"\"\" Perform 1000 times the sum between two 1000x1000 tensors and time it.\n",
        "    Repeat the operation a given number of times, returns the best (lowest)\n",
        "    execution time.\"\"\"\n",
        "    times = []\n",
        "    for i in range(num_trials):\n",
        "        a = torch.zeros(1000, 1000, device=device)\n",
        "        b = a.new_full(a.shape, 1.5)\n",
        "        tstart = time.time()\n",
        "        for i in range(1000):\n",
        "          a = a + b\n",
        "        times.append(time.time() - tstart)\n",
        "    return min(times)\n",
        "\n",
        "num_trials = 5\n",
        "best_time_cpu = expensive_operation(torch.device('cpu'), num_trials=num_trials)\n",
        "print('Best of {:d} trials, cpu: {:.5f}'.format(num_trials, best_time_cpu))\n",
        "\n",
        "best_time_gpu = expensive_operation(torch.device('cuda:0'),\n",
        "                                    num_trials=num_trials)\n",
        "print('Best of {:d} trials, gpu: {:.5f}'.format(num_trials, best_time_gpu))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYn2gx7JV_-i",
        "colab_type": "text"
      },
      "source": [
        "## Gradient tracking and back-propagation\n",
        "\n",
        "One of the feature of PyTorch Tensors is that they are able to keep track of all the operations that are performed with them and automatically compute the **gradient** of the operation. This is done using the **autograd** module.\n",
        "\n",
        "The gradient tracking can be activated when the Tensor is created or later, and stopped at any time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S6E7rdNOlRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "# Create a tensor and activate gradient tracking\n",
        "v = torch.ones(1, 2, requires_grad=True)\n",
        "# The gradient is stored in the grad attribute (empty at the start)\n",
        "print(v.grad)\n",
        "y = 3 * (v + 2)**2\n",
        "z = y.mean()\n",
        "# The gradients will be updated at each operation.\n",
        "# If you want to do something without keeping track of the gradients you can do:\n",
        "with torch.no_grad():\n",
        "    z *=2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iOBXiHOpD8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxzRAufepE1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's take a look at the gradient function graph using torchviz\n",
        "import torchviz\n",
        "torchviz.make_dot(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz6gpDjevdhK",
        "colab_type": "text"
      },
      "source": [
        "**autograd** is able to follow the graph and compute the gradient of z w.r.t any previous step by repeteadly applying the chain rule:\n",
        "\n",
        "$\\frac{\\partial z}{\\partial v} = \\frac{\\partial z}{\\partial y} \\cdot \\frac{\\partial y}{\\partial v}$\n",
        "\n",
        "(i have omitted the vector and tensor notation for simplicity)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38ZYFlRFptmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the chain of all gradients (using the chain rule) backward from z\n",
        "torch.autograd.backward(z)\n",
        "# or simply z.backward()\n",
        "# This will give us dz/dv, which in this case is equal to 3v + 6\n",
        "print(v.grad)\n",
        "# Zero all the gradients\n",
        "v.grad.zero_()\n",
        "print(v.grad)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_lGnz3qtvmW",
        "colab_type": "text"
      },
      "source": [
        "The most common use case for this feature is to the **Gradient Descent** training algorithm, which is one of the fundamental building blocks of many ML techniques. Usually, one defines a **loss** function (which is what the algorithm wants minimize) and compute the gradient of the loss function backwards from the network parameters at each step of the training.\n",
        "\n",
        "PyTorch, automating the gradient computation, makes this completely straightforward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHsfRECyu4h9",
        "colab_type": "text"
      },
      "source": [
        "## The nn and optimize modules\n",
        "\n",
        "We have introduced the low-level ingredients of PyTorch: tesnors, and automatic gradient computation.\n",
        "\n",
        "On top of that there is a whole library of pre-build classes and functions that are meant to be assembled to create ML models easily. You will see a few of them at work in the next lessons.\n",
        "\n",
        "The **torch.nn** and **torch.optimize** modules contain many building blocks for ML models. One the most fundamental is the **nn.module** class, which is meant to be a Base class for Neural Networks models.\n",
        "\n",
        "You can define your own NN by *inheriting* from nn.module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwLhPrQqLfux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "\"\"\" In order to inherit from nn.Module you have to define two class methods: \n",
        "1) The constructor (a.k.a. __init__)\n",
        "2) forward(), which defines what happens at each training step \"\"\"\n",
        "\n",
        "class MyModel(torch.nn.Module):\n",
        "    \"\"\" Example dummy class inheriting from nn.Module.\"\"\"\n",
        "    def __init__(self, *other_args):\n",
        "        # Call the constructor of nn.Module with the Python 3 'super' syntax\n",
        "        super(MyModel, self).__init__()\n",
        "        # Here goes the initialization code\n",
        "        pass\n",
        "    \n",
        "    def forward(self, input):\n",
        "        # Here you define what happens at each training step\n",
        "        pass\n",
        "\n",
        "# Create a model object by calling the constructor\n",
        "model = MyModel()\n",
        "# Let's take a look at what inheritng from nn.Module gives us for free\n",
        "print(dir(model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoWaXeUkQu4d",
        "colab_type": "text"
      },
      "source": [
        "That concludes our quick introduction to PyTorch."
      ]
    }
  ]
}